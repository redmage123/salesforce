{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ba921c",
   "metadata": {},
   "source": [
    "# Agent Assist for Salesforce Cases with RAG\n",
    "\n",
    "End-to-end prototype of \"Agent Assist on Case\" from salesforce_talks.docx.\n",
    "\n",
    "Pipeline:\n",
    "1. Ingest Knowledge (Knowledge__kav) articles (mock or via Salesforce) and embed chunks.\n",
    "2. Given a Case description, compute semantic similarity and retrieve top-k chunks.\n",
    "3. Construct a grounded prompt and call an LLM to draft a suggested response with citations.\n",
    "4. Log telemetry (latency, token estimates, costs) and visualize with matplotlib, seaborn, and plotly.\n",
    "\n",
    "You can run in two modes:\n",
    "- Mock mode (default): sample data, local embeddings via sentence-transformers, and a stub LLM.\n",
    "- Salesforce mode: pull Cases and Knowledge via simple_salesforce; call hosted embeddings/LLM APIs.\n",
    "\n",
    "Note: Replace API keys and uncomment the relevant code when using real services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cb7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy scikit-learn sentence-transformers simple-salesforce matplotlib seaborn plotly tqdm\n",
    "import os, time, json, math, textwrap, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a5229",
   "metadata": {},
   "source": [
    "## Config\n",
    "- USE_SF: set True to use Salesforce org (requires credentials).\n",
    "- For real APIs: set EMBEDDINGS_PROVIDER and LLM_PROVIDER to \"openai\" or your provider and supply keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7839932",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SF = False  # flip to True if using Salesforce org\n",
    "\n",
    "# Embeddings/LLM configuration\n",
    "EMBEDDINGS_BACKEND = \"local\"  # \"local\" | \"openai\" | \"cohere\" | \"azure_openai\"\n",
    "LLM_BACKEND = \"stub\"          # \"stub\" | \"openai\" | \"azure_openai\" | \"cohere\"\n",
    "\n",
    "# API keys (required for hosted providers)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\", \"\")\n",
    "\n",
    "# Retrieval parameters\n",
    "CHUNK_SIZE = 500  # characters\n",
    "CHUNK_OVERLAP = 100  # characters\n",
    "TOP_K = 4\n",
    "\n",
    "# Simple telemetry cost model (adjust for your provider)\n",
    "COST_PER_1K_INPUT_TOKENS = 0.0005\n",
    "COST_PER_1K_OUTPUT_TOKENS = 0.0015\n",
    "TOKENS_PER_CHAR = 0.25  # rough heuristic\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef958b",
   "metadata": {},
   "source": [
    "## Data access helpers\n",
    "When USE_SF=True, uses simple_salesforce to pull Cases and Knowledge.\n",
    "Otherwise, uses mock examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882d0930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            Id                             Subject  \\\n",
       " 0  500xx000001               Cannot reset password   \n",
       " 1  500xx000002  Email to Case not creating records   \n",
       " \n",
       "                                          Description  \n",
       " 0  User can't reset password using forgot link; g...  \n",
       " 1  Inbound emails are bouncing; Email-to-Case not...  ,\n",
       "            Id                           Title              UrlName  \\\n",
       " 0  ka0xx00001  Reset Password Troubleshooting       reset-password   \n",
       " 1  ka0xx00002       Email-to-Case Setup Guide  email-to-case-setup   \n",
       " 2  ka0xx00003      Login Issues Common Causes         login-issues   \n",
       " \n",
       "                           Summary  \\\n",
       " 0  Steps to resolve token issues.   \n",
       " 1      Configuring Email-to-Case.   \n",
       " 2          Common login problems.   \n",
       " \n",
       "                                          ArticleBody  \n",
       " 0  If users see an invalid token during password ...  \n",
       " 1  Verify routing addresses are active. Check the...  \n",
       " 2  Multi-factor prompts, IP ranges, and profile l...  )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_salesforce_data():\n",
    "    \"\"\"Example pull from Salesforce using simple_salesforce (uncomment and configure).\n",
    "    Returns two dataframes: df_cases, df_knowledge.\n",
    "    \"\"\"\n",
    "    # from simple_salesforce import Salesforce\n",
    "    # sf = Salesforce(\n",
    "    #     username=os.getenv(\"SF_USERNAME\"),\n",
    "    #     password=os.getenv(\"SF_PASSWORD\"),\n",
    "    #     security_token=os.getenv(\"SF_SECURITY_TOKEN\"),\n",
    "    #     domain=os.getenv(\"SF_DOMAIN\", \"login\")\n",
    "    # )\n",
    "    # soql_cases = \"SELECT Id, Subject, Description FROM Case WHERE Description != NULL LIMIT 50\"\n",
    "    # soql_kb = \"SELECT Id, Title, UrlName, Summary, ArticleBody FROM Knowledge__kav WHERE PublishStatus='Online' LIMIT 200\"\n",
    "    # cases = sf.query(soql_cases)[\"records\"]\n",
    "    # articles = sf.query(soql_kb)[\"records\"]\n",
    "    # df_cases = pd.DataFrame([{\"Id\": r[\"Id\"], \"Subject\": r.get(\"Subject\"), \"Description\": r.get(\"Description\")} for r in cases])\n",
    "    # df_kb = pd.DataFrame([{\n",
    "    #     \"Id\": r[\"Id\"],\n",
    "    #     \"Title\": r.get(\"Title\"),\n",
    "    #     \"UrlName\": r.get(\"UrlName\"),\n",
    "    #     \"Summary\": r.get(\"Summary\"),\n",
    "    #     \"ArticleBody\": r.get(\"ArticleBody\")\n",
    "    # } for r in articles])\n",
    "    # return df_cases, df_kb\n",
    "    raise NotImplementedError(\"Enable USE_SF and uncomment code to load real Salesforce data.\")\n",
    "\n",
    "def load_mock_data():\n",
    "    cases = [\n",
    "        {\"Id\": \"500xx000001\", \"Subject\": \"Cannot reset password\", \"Description\": \"User can't reset password using forgot link; gets invalid token error.\"},\n",
    "        {\"Id\": \"500xx000002\", \"Subject\": \"Email to Case not creating records\", \"Description\": \"Inbound emails are bouncing; Email-to-Case not generating cases since yesterday.\"}\n",
    "    ]\n",
    "    kb = [\n",
    "        {\"Id\": \"ka0xx00001\", \"Title\": \"Reset Password Troubleshooting\", \"UrlName\": \"reset-password\", \n",
    "         \"Summary\": \"Steps to resolve token issues.\",\n",
    "         \"ArticleBody\": \"If users see an invalid token during password reset, ensure the reset link has not expired (24 hours). Clear cache, request a new link, and check domain redirects. Admins can invalidate sessions and enforce a new reset.\"},\n",
    "        {\"Id\": \"ka0xx00002\", \"Title\": \"Email-to-Case Setup Guide\", \"UrlName\": \"email-to-case-setup\",\n",
    "         \"Summary\": \"Configuring Email-to-Case.\",\n",
    "         \"ArticleBody\": \"Verify routing addresses are active. Check the Email Services logs. Ensure the mailbox forwarding is enabled and not hitting size limits. Review Case Assignment Rules and APEX triggers that may block insert.\"},\n",
    "        {\"Id\": \"ka0xx00003\", \"Title\": \"Login Issues Common Causes\", \"UrlName\": \"login-issues\",\n",
    "         \"Summary\": \"Common login problems.\",\n",
    "         \"ArticleBody\": \"Multi-factor prompts, IP ranges, and profile login hours can block login. Review login history for error codes and investigate session settings.\"}\n",
    "    ]\n",
    "    return pd.DataFrame(cases), pd.DataFrame(kb)\n",
    "\n",
    "df_cases, df_kb = (load_salesforce_data() if USE_SF else load_mock_data())\n",
    "df_cases, df_kb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f18feb",
   "metadata": {},
   "source": [
    "## Chunking Knowledge Articles\n",
    "Split ArticleBody into overlapping chunks to improve retrieval granularity and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133eb607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Title</th>\n",
       "      <th>UrlName</th>\n",
       "      <th>ChunkIndex</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ka0xx00001</td>\n",
       "      <td>Reset Password Troubleshooting</td>\n",
       "      <td>reset-password</td>\n",
       "      <td>0</td>\n",
       "      <td>If users see an invalid token during password ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ka0xx00002</td>\n",
       "      <td>Email-to-Case Setup Guide</td>\n",
       "      <td>email-to-case-setup</td>\n",
       "      <td>0</td>\n",
       "      <td>Verify routing addresses are active. Check the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ka0xx00003</td>\n",
       "      <td>Login Issues Common Causes</td>\n",
       "      <td>login-issues</td>\n",
       "      <td>0</td>\n",
       "      <td>Multi-factor prompts, IP ranges, and profile l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ArticleId                           Title              UrlName  \\\n",
       "0  ka0xx00001  Reset Password Troubleshooting       reset-password   \n",
       "1  ka0xx00002       Email-to-Case Setup Guide  email-to-case-setup   \n",
       "2  ka0xx00003      Login Issues Common Causes         login-issues   \n",
       "\n",
       "   ChunkIndex                                              Chunk  \n",
       "0           0  If users see an invalid token during password ...  \n",
       "1           0  Verify routing addresses are active. Check the...  \n",
       "2           0  Multi-factor prompts, IP ranges, and profile l...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):\n",
    "    text = text or \"\"\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunk = text[i:i+size]\n",
    "        chunks.append(chunk)\n",
    "        i += max(1, size - overlap)\n",
    "    return chunks\n",
    "\n",
    "kb_rows = []\n",
    "for _, row in df_kb.iterrows():\n",
    "    chunks = chunk_text(row.get(\"ArticleBody\", \"\"))\n",
    "    for ci, ch in enumerate(chunks):\n",
    "        kb_rows.append({\n",
    "            \"ArticleId\": row[\"Id\"],\n",
    "            \"Title\": row.get(\"Title\", \"\"),\n",
    "            \"UrlName\": row.get(\"UrlName\", \"\"),\n",
    "            \"ChunkIndex\": ci,\n",
    "            \"Chunk\": ch\n",
    "        })\n",
    "df_chunks = pd.DataFrame(kb_rows)\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bb567",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Use local embeddings via sentence-transformers by default.\n",
    "Swap to hosted embeddings by setting EMBEDDINGS_BACKEND and providing API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26c44f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported EMBEDDINGS_BACKEND\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m chunk_embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39membed(df_chunks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     47\u001b[0m chunk_embeddings\u001b[38;5;241m.\u001b[39mshape\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mget_embedder\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedder\u001b[39m():\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m EMBEDDINGS_BACKEND \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m EMBEDDINGS_BACKEND \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mLocalEmbedder.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "class LocalEmbedder:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        return np.array(self.model.encode(texts, batch_size=32, show_progress_bar=False))\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_embedder():\n",
    "    if EMBEDDINGS_BACKEND == \"local\":\n",
    "        return LocalEmbedder()\n",
    "    elif EMBEDDINGS_BACKEND == \"openai\":\n",
    "        import openai\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        class OpenAIEmbedder:\n",
    "            def embed(self, texts: List[str]) -> np.ndarray:\n",
    "                resp = openai.embeddings.create(model=\"text-embedding-3-small\", input=texts)\n",
    "                vecs = [d[\"embedding\"] for d in resp.data]\n",
    "                return np.array(vecs)\n",
    "        return OpenAIEmbedder()\n",
    "    elif EMBEDDINGS_BACKEND == \"cohere\":\n",
    "        import cohere\n",
    "        co = cohere.Client(COHERE_API_KEY)\n",
    "        class CohereEmbedder:\n",
    "            def embed(self, texts: List[str]) -> np.ndarray:\n",
    "                resp = co.embed(texts=texts, model=\"embed-english-v3.0\")\n",
    "                return np.array(resp.embeddings)\n",
    "        return CohereEmbedder()\n",
    "    elif EMBEDDINGS_BACKEND == \"azure_openai\":\n",
    "        # Example for Azure OpenAI Embeddings (adapt as needed)\n",
    "        from openai import AzureOpenAI\n",
    "        client = AzureOpenAI(api_key=AZURE_OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=\"2024-02-15-preview\")\n",
    "        class AzureEmbedder:\n",
    "            def embed(self, texts: List[str]) -> np.ndarray:\n",
    "                resp = client.embeddings.create(model=\"text-embedding-3-small\", input=texts)\n",
    "                vecs = [d.embedding for d in resp.data]\n",
    "                return np.array(vecs)\n",
    "        return AzureEmbedder()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported EMBEDDINGS_BACKEND\")\n",
    "\n",
    "embedder = get_embedder()\n",
    "chunk_embeddings = embedder.embed(df_chunks[\"Chunk\"].tolist())\n",
    "chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2f942",
   "metadata": {},
   "source": [
    "## Retrieve top-k chunks for a Case\n",
    "Compute similarity between Case description and all Knowledge chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve(case_text: str, top_k: int = TOP_K):\n",
    "    qvec = embedder.embed([case_text])\n",
    "    sims = cosine_similarity(qvec, chunk_embeddings)[0]\n",
    "    idx = np.argsort(-sims)[:top_k]\n",
    "    results = []\n",
    "    for i in idx:\n",
    "        row = df_chunks.iloc[i]\n",
    "        results.append({\n",
    "            \"ArticleId\": row[\"ArticleId\"],\n",
    "            \"Title\": row[\"Title\"],\n",
    "            \"UrlName\": row[\"UrlName\"],\n",
    "            \"ChunkIndex\": int(row[\"ChunkIndex\"]),\n",
    "            \"Chunk\": row[\"Chunk\"],\n",
    "            \"Similarity\": float(sims[i])\n",
    "        })\n",
    "    return results\n",
    "\n",
    "sample_case = df_cases.iloc[0]\n",
    "retrieved = retrieve(sample_case[\"Description\"], top_k=TOP_K)\n",
    "pd.DataFrame(retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4b069",
   "metadata": {},
   "source": [
    "## Grounded generation (LLM)\n",
    "Use a stub LLM by default that composes an answer from retrieved snippets.\n",
    "Switch to a hosted LLM by setting LLM_BACKEND and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    return int(len(text) * TOKENS_PER_CHAR)\n",
    "\n",
    "def estimate_cost(input_tokens: int, output_tokens: int) -> float:\n",
    "    return (input_tokens/1000) * COST_PER_1K_INPUT_TOKENS + (output_tokens/1000) * COST_PER_1K_OUTPUT_TOKENS\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Salesforce support assistant. Answer the agent's question using ONLY the provided Knowledge snippets. \"\n",
    "    \"Cite the article titles or UrlName as [cite:Title] in-line. If unsure, say you don't have enough information.\"\n",
    ")\n",
    "\n",
    "def call_llm_stub(prompt: str) -> str:\n",
    "    # Very simple abstractive summary from retrieved snippets\n",
    "    lines = [l.strip() for l in prompt.splitlines() if l.strip()]\n",
    "    snippets = [l for l in lines if l.lower().startswith(\"snippet \")]\n",
    "    guidance = []\n",
    "    for s in snippets:\n",
    "        if \"invalid token\" in s.lower():\n",
    "            guidance.append(\"Ensure reset link not expired; request new link; clear cache; check redirects [cite:Reset Password Troubleshooting].\")\n",
    "        if \"email\" in s.lower() and \"case\" in s.lower():\n",
    "            guidance.append(\"Verify routing addresses and forwarding; check Email Services logs; ensure mailbox not over quota [cite:Email-to-Case Setup Guide].\")\n",
    "    if not guidance:\n",
    "        guidance.append(\"I don't have enough information to provide a grounded answer.\")\n",
    "    return \" \".join(guidance)\n",
    "\n",
    "def call_llm_hosted(prompt: str) -> str:\n",
    "    if LLM_BACKEND == \"openai\":\n",
    "        import openai\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},{\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    elif LLM_BACKEND == \"cohere\":\n",
    "        import cohere\n",
    "        co = cohere.Client(COHERE_API_KEY)\n",
    "        resp = co.chat(model=\"command-r-plus\", messages=[{\"role\":\"SYSTEM\",\"content\":SYSTEM_PROMPT},{\"role\":\"USER\",\"content\":prompt}], temperature=0.2)\n",
    "        return resp.text\n",
    "    elif LLM_BACKEND == \"azure_openai\":\n",
    "        from openai import AzureOpenAI\n",
    "        client = AzureOpenAI(api_key=AZURE_OPENAI_API_KEY, azure_endpoint=AZURE_OPENAI_ENDPOINT, api_version=\"2024-02-15-preview\")\n",
    "        resp = client.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},{\"role\":\"user\",\"content\":prompt}], temperature=0.2)\n",
    "        return resp.choices[0].message.content\n",
    "    else:\n",
    "        return call_llm_stub(prompt)\n",
    "\n",
    "def build_prompt(case_subject: str, case_description: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "    header = f\"Case: {case_subject}\\nQuestion: {case_description}\\n\\n\"\n",
    "    context_lines = []\n",
    "    for i, r in enumerate(retrieved_chunks, start=1):\n",
    "        title = r.get(\"Title\") or r.get(\"UrlName\")\n",
    "        snippet = textwrap.shorten(r[\"Chunk\"], width=350, placeholder=\"...\")\n",
    "        context_lines.append(f\"Snippet {i} [{title}]: {snippet}\")\n",
    "    return header + \"\\n\".join(context_lines)\n",
    "\n",
    "def answer_case(case_row, retrieved_chunks):\n",
    "    prompt = build_prompt(case_row.get(\"Subject\", \"\"), case_row.get(\"Description\", \"\"), retrieved_chunks)\n",
    "    t0 = time.time()\n",
    "    response = call_llm_hosted(prompt) if LLM_BACKEND != \"stub\" else call_llm_stub(prompt)\n",
    "    latency = (time.time() - t0) * 1000\n",
    "    input_tokens = estimate_tokens(SYSTEM_PROMPT + \"\\n\" + prompt)\n",
    "    output_tokens = estimate_tokens(response)\n",
    "    cost = estimate_cost(input_tokens, output_tokens)\n",
    "    return {\n",
    "        \"Response\": response,\n",
    "        \"Prompt\": prompt,\n",
    "        \"LatencyMs\": latency,\n",
    "        \"InputTokens\": input_tokens,\n",
    "        \"OutputTokens\": output_tokens,\n",
    "        \"CostUSD\": cost,\n",
    "    }\n",
    "\n",
    "result = answer_case(sample_case, retrieved)\n",
    "result[\"Response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c4d2c",
   "metadata": {},
   "source": [
    "## Telemetry log and evaluation placeholders\n",
    "We’ll collect telemetry per run and provide a simple quality rubric you can adapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "telemetry: List[Dict[str, Any]] = []\n",
    "\n",
    "def log_run(case_id: str, retrieved_chunks: List[Dict[str, Any]], result: Dict[str, Any], label_quality: int = None):\n",
    "    import numpy as np\n",
    "    tel = {\n",
    "        \"CaseId\": case_id,\n",
    "        \"TopK\": len(retrieved_chunks),\n",
    "        \"AvgSim\": float(np.mean([x[\"Similarity\"] for x in retrieved_chunks])) if retrieved_chunks else 0.0,\n",
    "        **result,\n",
    "        \"QualityLabel\": label_quality  # optional manual rating 1–5\n",
    "    }\n",
    "    telemetry.append(tel)\n",
    "    return tel\n",
    "\n",
    "tel_row = log_run(sample_case[\"Id\"], retrieved, result, label_quality=4)\n",
    "pd.DataFrame([tel_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7536e8",
   "metadata": {},
   "source": [
    "## Batch over multiple Cases (mock) and visualize\n",
    "We’ll run the pipeline for a few cases and visualize cost, latency, and similarity vs. quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, c in df_cases.iterrows():\n",
    "    r = retrieve(c[\"Description\"], top_k=TOP_K)\n",
    "    res = answer_case(c, r)\n",
    "    # Simulate a manual quality rating (replace with real eval rubric)\n",
    "    quality = 5 if \"password\" in c[\"Subject\"].lower() else 3\n",
    "    log_run(c[\"Id\"], r, res, label_quality=quality)\n",
    "\n",
    "df_tel = pd.DataFrame(telemetry)\n",
    "df_tel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d2e0d",
   "metadata": {},
   "source": [
    "### Matplotlib: Latency distribution and Cost vs Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax[0].hist(df_tel[\"LatencyMs\"], bins=5, color=\"#4C78A8\")\n",
    "ax[0].set_title(\"Latency (ms)\")\n",
    "ax[0].set_xlabel(\"ms\"); ax[0].set_ylabel(\"count\")\n",
    "\n",
    "ax[1].scatter(df_tel[\"InputTokens\"]+df_tel[\"OutputTokens\"], df_tel[\"CostUSD\"], color=\"#F58518\")\n",
    "ax[1].set_title(\"Cost vs Total Tokens\")\n",
    "ax[1].set_xlabel(\"tokens\"); ax[1].set_ylabel(\"USD\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963cc42",
   "metadata": {},
   "source": [
    "### Seaborn: Similarity vs Quality label (mock)\n",
    "Helps assess whether retrieval quality correlates with perceived answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.regplot(data=df_tel, x=\"AvgSim\", y=\"QualityLabel\", scatter_kws={\"s\":70})\n",
    "plt.title(\"Avg Similarity vs Quality Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ece3e2",
   "metadata": {},
   "source": [
    "### Plotly: Interactive telemetry dashboard\n",
    "Explore per-case latency and cost interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_tel, x=\"LatencyMs\", y=\"CostUSD\", color=\"QualityLabel\", hover_data=[\"CaseId\", \"InputTokens\", \"OutputTokens\"], title=\"Latency vs Cost (colored by Quality)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221bb20",
   "metadata": {},
   "source": [
    "## Inspect a single generated response with citations\n",
    "You can adapt the prompt format to include explicit citation tags and map UrlName to Salesforce record URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e58762",
   "metadata": {},
   "outputs": [],
   "source": [
    "case0 = df_cases.iloc[0]\n",
    "r0 = retrieve(case0[\"Description\"], top_k=TOP_K)\n",
    "res0 = answer_case(case0, r0)\n",
    "print(\"=== Suggested Response ===\\n\", res0[\"Response\"])\n",
    "print(\"\\n=== Prompt (context) ===\\n\", res0[\"Prompt\"])\n",
    "pd.DataFrame(r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded2f6a",
   "metadata": {},
   "source": [
    "## Next steps (to production)\n",
    "- Replace local embeddings/LLM stub with your chosen providers and log real token usage/costs.\n",
    "- Externalize the vector index (e.g., pgvector on Heroku) and hit it via a secure endpoint.\n",
    "- Store prompt and model versions, and log telemetry to a durable store (e.g., Postgres or Salesforce Custom Object).\n",
    "- Add safety/PII redaction before sending any Case content to external services.\n",
    "- Package as a service callable from an LWC in the Case page; return {answer, citations[]}.\n",
    "- Add evaluation harness with labeled Case->Answer pairs for continuous quality monitoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
