{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Implementing & Integrating AI into Your Applications\n",
    "\n",
    "## Complete Guide with Live Demo\n",
    "\n",
    "This notebook demonstrates practical patterns for integrating AI into enterprise applications, including:\n",
    "\n",
    "1. **API Integration Patterns** - OpenAI, Anthropic, Azure OpenAI\n",
    "2. **RAG (Retrieval-Augmented Generation)** - Context-aware responses\n",
    "3. **Prompt Engineering** - Effective prompt design\n",
    "4. **Error Handling & Resilience** - Retries, fallbacks, circuit breakers\n",
    "5. **Cost Management** - Token tracking and budgets\n",
    "6. **Security & Safety** - PII redaction, content filtering\n",
    "7. **Production Patterns** - Caching, rate limiting, monitoring\n",
    "\n",
    "**Live Demo**: Customer support assistant with RAG, safety checks, and real-time analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "install",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ openai already installed\n",
      "✓ anthropic already installed\n",
      "✓ tiktoken already installed\n",
      "✓ sentence-transformers already installed\n",
      "✓ chromadb already installed\n",
      "✓ pandas already installed\n",
      "✓ numpy already installed\n",
      "✓ plotly already installed\n",
      "Installing python-dotenv...\n",
      "Requirement already satisfied: python-dotenv in /home/bbrelin/src/repos/salesforce/.venv/lib/python3.12/site-packages (1.1.1)\n",
      "✓ python-dotenv installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'openai',\n",
    "    'anthropic',\n",
    "    'tiktoken',\n",
    "    'sentence-transformers',\n",
    "    'chromadb',\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'plotly',\n",
    "    'python-dotenv'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"✓ {package} installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern1",
   "metadata": {},
   "source": [
    "## 1. AI Provider Integration Patterns\n",
    "\n",
    "### Multi-Provider Architecture\n",
    "\n",
    "Build a unified interface that supports multiple AI providers for flexibility and failover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "provider_interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AI Provider interfaces defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AIMessage:\n",
    "    \"\"\"Standardized message format\"\"\"\n",
    "    role: str  # 'system', 'user', 'assistant'\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class AIResponse:\n",
    "    \"\"\"Standardized response format\"\"\"\n",
    "    content: str\n",
    "    model: str\n",
    "    provider: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    cost: float\n",
    "    latency_ms: float\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class AIProviderInterface:\n",
    "    \"\"\"Abstract base for AI providers\"\"\"\n",
    "    \n",
    "    def complete(\n",
    "        self,\n",
    "        messages: List[AIMessage],\n",
    "        model: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1000\n",
    "    ) -> AIResponse:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpenAIProvider(AIProviderInterface):\n",
    "    \"\"\"OpenAI integration\"\"\"\n",
    "    \n",
    "    # Cost per 1M tokens (as of 2025)\n",
    "    PRICING = {\n",
    "        'gpt-4o': {'input': 2.50, 'output': 10.00},\n",
    "        'gpt-4o-mini': {'input': 0.150, 'output': 0.600},\n",
    "        'gpt-4-turbo': {'input': 10.00, 'output': 30.00},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))\n",
    "    \n",
    "    def complete(self, messages: List[AIMessage], model: str = 'gpt-4o-mini',\n",
    "                 temperature: float = 0.7, max_tokens: int = 1000) -> AIResponse:\n",
    "        start = time.time()\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{'role': m.role, 'content': m.content} for m in messages],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        usage = response.usage\n",
    "        \n",
    "        # Calculate cost\n",
    "        pricing = self.PRICING.get(model, self.PRICING['gpt-4o-mini'])\n",
    "        cost = (\n",
    "            (usage.prompt_tokens / 1_000_000) * pricing['input'] +\n",
    "            (usage.completion_tokens / 1_000_000) * pricing['output']\n",
    "        )\n",
    "        \n",
    "        return AIResponse(\n",
    "            content=response.choices[0].message.content,\n",
    "            model=model,\n",
    "            provider='openai',\n",
    "            prompt_tokens=usage.prompt_tokens,\n",
    "            completion_tokens=usage.completion_tokens,\n",
    "            total_tokens=usage.total_tokens,\n",
    "            cost=cost,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "\n",
    "class AnthropicProvider(AIProviderInterface):\n",
    "    \"\"\"Anthropic Claude integration\"\"\"\n",
    "    \n",
    "    PRICING = {\n",
    "        'claude-3-5-sonnet-20241022': {'input': 3.00, 'output': 15.00},\n",
    "        'claude-3-5-haiku-20241022': {'input': 0.80, 'output': 4.00},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        from anthropic import Anthropic\n",
    "        self.client = Anthropic(api_key=api_key or os.getenv('ANTHROPIC_API_KEY'))\n",
    "    \n",
    "    def complete(self, messages: List[AIMessage], model: str = 'claude-3-5-haiku-20241022',\n",
    "                 temperature: float = 0.7, max_tokens: int = 1000) -> AIResponse:\n",
    "        start = time.time()\n",
    "        \n",
    "        # Separate system message from conversation\n",
    "        system_msg = next((m.content for m in messages if m.role == 'system'), None)\n",
    "        conv_messages = [{'role': m.role, 'content': m.content} \n",
    "                        for m in messages if m.role != 'system']\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=model,\n",
    "            system=system_msg,\n",
    "            messages=conv_messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        \n",
    "        # Calculate cost\n",
    "        pricing = self.PRICING.get(model, self.PRICING['claude-3-5-haiku-20241022'])\n",
    "        cost = (\n",
    "            (response.usage.input_tokens / 1_000_000) * pricing['input'] +\n",
    "            (response.usage.output_tokens / 1_000_000) * pricing['output']\n",
    "        )\n",
    "        \n",
    "        return AIResponse(\n",
    "            content=response.content[0].text,\n",
    "            model=model,\n",
    "            provider='anthropic',\n",
    "            prompt_tokens=response.usage.input_tokens,\n",
    "            completion_tokens=response.usage.output_tokens,\n",
    "            total_tokens=response.usage.input_tokens + response.usage.output_tokens,\n",
    "            cost=cost,\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "\n",
    "print(\"✅ AI Provider interfaces defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern2",
   "metadata": {},
   "source": [
    "## 2. Resilience Patterns\n",
    "\n",
    "### Retry Logic, Circuit Breaker, and Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "resilience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resilience patterns implemented\n"
     ]
    }
   ],
   "source": [
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker pattern for AI API calls\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, timeout: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = 'closed'  # closed, open, half-open\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        if self.state == 'open':\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = 'half-open'\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self.on_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.on_failure()\n",
    "            raise e\n",
    "    \n",
    "    def on_success(self):\n",
    "        self.failures = 0\n",
    "        self.state = 'closed'\n",
    "    \n",
    "    def on_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failures >= self.failure_threshold:\n",
    "            self.state = 'open'\n",
    "\n",
    "def with_retry(max_retries: int = 3, backoff: float = 1.0):\n",
    "    \"\"\"Retry decorator with exponential backoff\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        raise e\n",
    "                    wait_time = backoff * (2 ** attempt)\n",
    "                    print(f\"⚠️  Retry {attempt + 1}/{max_retries} after {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class AIClient:\n",
    "    \"\"\"Resilient AI client with retry, circuit breaker, and fallback\"\"\"\n",
    "    \n",
    "    def __init__(self, primary_provider: AIProviderInterface,\n",
    "                 fallback_provider: Optional[AIProviderInterface] = None):\n",
    "        self.primary = primary_provider\n",
    "        self.fallback = fallback_provider\n",
    "        self.circuit_breaker = CircuitBreaker()\n",
    "    \n",
    "    @with_retry(max_retries=3, backoff=1.0)\n",
    "    def complete(self, messages: List[AIMessage], **kwargs) -> AIResponse:\n",
    "        try:\n",
    "            return self.circuit_breaker.call(\n",
    "                self.primary.complete,\n",
    "                messages,\n",
    "                **kwargs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if self.fallback:\n",
    "                print(f\"⚠️  Primary failed, using fallback: {e}\")\n",
    "                return self.fallback.complete(messages, **kwargs)\n",
    "            raise e\n",
    "\n",
    "print(\"✅ Resilience patterns implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern3",
   "metadata": {},
   "source": [
    "## 3. RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "### Vector Store & Context Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "rag",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG system implemented\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "class KnowledgeBase:\n",
    "    \"\"\"Vector-based knowledge base for RAG\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"knowledge\"):\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        # Use sentence transformers for embeddings\n",
    "        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=\"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_fn\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Add documents to knowledge base\n",
    "        \n",
    "        Args:\n",
    "            documents: List of {'id': str, 'text': str, 'metadata': dict}\n",
    "        \"\"\"\n",
    "        self.collection.add(\n",
    "            ids=[doc['id'] for doc in documents],\n",
    "            documents=[doc['text'] for doc in documents],\n",
    "            metadatas=[doc.get('metadata', {}) for doc in documents]\n",
    "        )\n",
    "        print(f\"✅ Added {len(documents)} documents to knowledge base\")\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for relevant documents\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'text': doc,\n",
    "                'metadata': meta,\n",
    "                'distance': dist\n",
    "            }\n",
    "            for doc, meta, dist in zip(\n",
    "                results['documents'][0],\n",
    "                results['metadatas'][0],\n",
    "                results['distances'][0]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "class RAGAssistant:\n",
    "    \"\"\"AI assistant with RAG capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, ai_client: AIClient, knowledge_base: KnowledgeBase):\n",
    "        self.ai_client = ai_client\n",
    "        self.kb = knowledge_base\n",
    "    \n",
    "    def answer_question(self, question: str, system_prompt: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Answer question using RAG\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        context_docs = self.kb.search(question, n_results=3)\n",
    "        \n",
    "        # Build context string\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {doc['text']}\"\n",
    "            for i, doc in enumerate(context_docs)\n",
    "        ])\n",
    "        \n",
    "        # Build prompt with context\n",
    "        messages = []\n",
    "        \n",
    "        if system_prompt:\n",
    "            messages.append(AIMessage(\n",
    "                role='system',\n",
    "                content=system_prompt\n",
    "            ))\n",
    "        \n",
    "        messages.append(AIMessage(\n",
    "            role='user',\n",
    "            content=f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question using the provided context. Cite sources using [Source N] format.\"\"\"\n",
    "        ))\n",
    "        \n",
    "        # Get AI response\n",
    "        response = self.ai_client.complete(messages, max_tokens=500)\n",
    "        \n",
    "        return {\n",
    "            'answer': response.content,\n",
    "            'sources': context_docs,\n",
    "            'cost': response.cost,\n",
    "            'latency_ms': response.latency_ms,\n",
    "            'tokens': response.total_tokens\n",
    "        }\n",
    "\n",
    "print(\"✅ RAG system implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern4",
   "metadata": {},
   "source": [
    "## 4. Security & Safety\n",
    "\n",
    "### PII Redaction, Content Filtering, Input Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Security features implemented\n"
     ]
    }
   ],
   "source": [
    "class SafetyFilter:\n",
    "    \"\"\"Content safety and PII redaction\"\"\"\n",
    "    \n",
    "    # PII patterns\n",
    "    PATTERNS = {\n",
    "        'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "        'phone': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "        'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
    "        'credit_card': r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b',\n",
    "    }\n",
    "    \n",
    "    # Blocked content keywords\n",
    "    BLOCKED_KEYWORDS = [\n",
    "        'hack', 'exploit', 'vulnerability', 'malware',\n",
    "        'illegal', 'fraud', 'scam'\n",
    "    ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def redact_pii(text: str) -> tuple[str, List[str]]:\n",
    "        \"\"\"Redact PII from text\n",
    "        \n",
    "        Returns:\n",
    "            (redacted_text, list of PII types found)\n",
    "        \"\"\"\n",
    "        redacted = text\n",
    "        found_pii = []\n",
    "        \n",
    "        for pii_type, pattern in SafetyFilter.PATTERNS.items():\n",
    "            matches = re.findall(pattern, redacted)\n",
    "            if matches:\n",
    "                found_pii.append(pii_type)\n",
    "                redacted = re.sub(pattern, f'[REDACTED_{pii_type.upper()}]', redacted)\n",
    "        \n",
    "        return redacted, found_pii\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_content(text: str) -> tuple[bool, List[str]]:\n",
    "        \"\"\"Check for blocked content\n",
    "        \n",
    "        Returns:\n",
    "            (is_safe, list of blocked keywords found)\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "        found_keywords = [\n",
    "            kw for kw in SafetyFilter.BLOCKED_KEYWORDS\n",
    "            if kw in text_lower\n",
    "        ]\n",
    "        \n",
    "        return len(found_keywords) == 0, found_keywords\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_input(text: str, max_length: int = 10000) -> tuple[bool, str]:\n",
    "        \"\"\"Validate user input\n",
    "        \n",
    "        Returns:\n",
    "            (is_valid, error_message)\n",
    "        \"\"\"\n",
    "        if not text or not text.strip():\n",
    "            return False, \"Input cannot be empty\"\n",
    "        \n",
    "        if len(text) > max_length:\n",
    "            return False, f\"Input too long (max {max_length} chars)\"\n",
    "        \n",
    "        return True, \"\"\n",
    "\n",
    "class SecureAIAssistant(RAGAssistant):\n",
    "    \"\"\"AI assistant with security features\"\"\"\n",
    "    \n",
    "    def answer_question(self, question: str, system_prompt: str = None,\n",
    "                       redact_pii: bool = True) -> Dict[str, Any]:\n",
    "        # Validate input\n",
    "        is_valid, error = SafetyFilter.validate_input(question)\n",
    "        if not is_valid:\n",
    "            return {'error': error, 'answer': None}\n",
    "        \n",
    "        # Check content safety\n",
    "        is_safe, blocked = SafetyFilter.check_content(question)\n",
    "        if not is_safe:\n",
    "            return {\n",
    "                'error': f'Blocked content detected: {blocked}',\n",
    "                'answer': None\n",
    "            }\n",
    "        \n",
    "        # Redact PII if enabled\n",
    "        processed_question = question\n",
    "        pii_found = []\n",
    "        if redact_pii:\n",
    "            processed_question, pii_found = SafetyFilter.redact_pii(question)\n",
    "        \n",
    "        # Get answer from parent class\n",
    "        result = super().answer_question(processed_question, system_prompt)\n",
    "        \n",
    "        # Add security metadata\n",
    "        result['security'] = {\n",
    "            'pii_redacted': pii_found,\n",
    "            'content_safe': is_safe\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ Security features implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern5",
   "metadata": {},
   "source": [
    "## 5. Cost Management & Analytics\n",
    "\n",
    "### Token Tracking, Budgets, and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cost_management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cost management system implemented\n"
     ]
    }
   ],
   "source": [
    "class CostTracker:\n",
    "    \"\"\"Track and manage AI API costs\"\"\"\n",
    "    \n",
    "    def __init__(self, daily_budget: float = 10.0, monthly_budget: float = 200.0):\n",
    "        self.daily_budget = daily_budget\n",
    "        self.monthly_budget = monthly_budget\n",
    "        self.calls = []\n",
    "    \n",
    "    def log_call(self, response: AIResponse):\n",
    "        \"\"\"Log an API call\"\"\"\n",
    "        self.calls.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'provider': response.provider,\n",
    "            'model': response.model,\n",
    "            'prompt_tokens': response.prompt_tokens,\n",
    "            'completion_tokens': response.completion_tokens,\n",
    "            'total_tokens': response.total_tokens,\n",
    "            'cost': response.cost,\n",
    "            'latency_ms': response.latency_ms\n",
    "        })\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cost and usage statistics\"\"\"\n",
    "        if not self.calls:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.calls)\n",
    "        \n",
    "        # Today's costs\n",
    "        today = datetime.now().date()\n",
    "        today_calls = df[df['timestamp'].dt.date == today]\n",
    "        \n",
    "        # This month's costs\n",
    "        this_month = datetime.now().replace(day=1).date()\n",
    "        month_calls = df[df['timestamp'].dt.date >= this_month]\n",
    "        \n",
    "        return {\n",
    "            'total_calls': len(df),\n",
    "            'total_cost': df['cost'].sum(),\n",
    "            'total_tokens': df['total_tokens'].sum(),\n",
    "            'avg_latency_ms': df['latency_ms'].mean(),\n",
    "            'today_cost': today_calls['cost'].sum(),\n",
    "            'today_budget_remaining': self.daily_budget - today_calls['cost'].sum(),\n",
    "            'month_cost': month_calls['cost'].sum(),\n",
    "            'month_budget_remaining': self.monthly_budget - month_calls['cost'].sum(),\n",
    "            'by_provider': df.groupby('provider')['cost'].sum().to_dict(),\n",
    "            'by_model': df.groupby('model')['cost'].sum().to_dict()\n",
    "        }\n",
    "    \n",
    "    def check_budget(self) -> tuple[bool, str]:\n",
    "        \"\"\"Check if within budget\n",
    "        \n",
    "        Returns:\n",
    "            (within_budget, message)\n",
    "        \"\"\"\n",
    "        stats = self.get_stats()\n",
    "        \n",
    "        if stats.get('today_budget_remaining', self.daily_budget) < 0:\n",
    "            return False, \"Daily budget exceeded\"\n",
    "        \n",
    "        if stats.get('month_budget_remaining', self.monthly_budget) < 0:\n",
    "            return False, \"Monthly budget exceeded\"\n",
    "        \n",
    "        return True, \"Within budget\"\n",
    "    \n",
    "    def visualize_costs(self):\n",
    "        \"\"\"Create interactive cost visualization\"\"\"\n",
    "        if not self.calls:\n",
    "            print(\"No data to visualize\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.calls)\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        \n",
    "        # Cost over time\n",
    "        daily_cost = df.groupby('date')['cost'].sum().reset_index()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=daily_cost['date'],\n",
    "            y=daily_cost['cost'],\n",
    "            mode='lines+markers',\n",
    "            name='Daily Cost',\n",
    "            line=dict(color='#1f77b4', width=2)\n",
    "        ))\n",
    "        \n",
    "        # Budget line\n",
    "        fig.add_hline(\n",
    "            y=self.daily_budget,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            annotation_text=\"Daily Budget\"\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='AI API Costs Over Time',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Cost ($)',\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "print(\"✅ Cost management system implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_setup",
   "metadata": {},
   "source": [
    "## Live Demo: Customer Support Assistant\n",
    "\n",
    "Let's build a complete AI-powered customer support assistant with:\n",
    "- RAG for knowledge retrieval\n",
    "- PII redaction\n",
    "- Cost tracking\n",
    "- Multi-provider failover\n",
    "- Real-time analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "demo_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 8 documents to knowledge base\n",
      "✅ Knowledge base populated with support articles\n"
     ]
    }
   ],
   "source": [
    "# Initialize knowledge base with sample support articles\n",
    "kb = KnowledgeBase(collection_name=\"support_kb\")\n",
    "\n",
    "support_articles = [\n",
    "    {\n",
    "        'id': 'art-001',\n",
    "        'text': \"\"\"Password Reset Process: To reset your password, click 'Forgot Password' on the login page. \n",
    "        Enter your email address and you'll receive a reset link within 5 minutes. The link expires after 24 hours. \n",
    "        If you don't receive the email, check your spam folder or contact support.\"\"\",\n",
    "        'metadata': {'category': 'authentication', 'priority': 'high'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-002',\n",
    "        'text': \"\"\"Account Setup: New accounts are activated within 1 business day. You'll receive a welcome email \n",
    "        with your login credentials. First-time users should complete the onboarding tutorial to learn about \n",
    "        key features. Admin users can invite team members from the Settings > Users page.\"\"\",\n",
    "        'metadata': {'category': 'onboarding', 'priority': 'medium'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-003',\n",
    "        'text': \"\"\"Billing Questions: Billing occurs on the 1st of each month for your subscription tier. \n",
    "        You can view invoices in Settings > Billing. To upgrade or downgrade, contact your account manager. \n",
    "        Refunds are available within 30 days of purchase. Payment methods include credit card and wire transfer.\"\"\",\n",
    "        'metadata': {'category': 'billing', 'priority': 'high'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-004',\n",
    "        'text': \"\"\"API Integration: Our REST API is available at https://api.example.com/v1. Authentication uses \n",
    "        API keys generated in Settings > API Keys. Rate limits are 1000 requests/hour for standard tier and \n",
    "        10,000 requests/hour for enterprise. Full API documentation is at https://docs.example.com/api.\"\"\",\n",
    "        'metadata': {'category': 'technical', 'priority': 'medium'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-005',\n",
    "        'text': \"\"\"Data Export: You can export your data anytime from Settings > Data Export. Choose between \n",
    "        CSV, JSON, or Excel formats. Large exports are processed asynchronously and you'll receive a download \n",
    "        link via email. Data retention policy keeps exports available for 7 days.\"\"\",\n",
    "        'metadata': {'category': 'data', 'priority': 'low'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-006',\n",
    "        'text': \"\"\"Two-Factor Authentication: Enable 2FA in Settings > Security for enhanced account protection. \n",
    "        We support authenticator apps (Google Authenticator, Authy) and SMS codes. Keep backup codes in a safe \n",
    "        place. If you lose access to your 2FA device, contact support with verification.\"\"\",\n",
    "        'metadata': {'category': 'security', 'priority': 'high'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-007',\n",
    "        'text': \"\"\"Performance Issues: If experiencing slow performance, try clearing your browser cache and cookies. \n",
    "        Check your internet connection speed (minimum 5 Mbps recommended). Disable browser extensions that might \n",
    "        interfere. For persistent issues, send us a HAR file from your browser's developer tools.\"\"\",\n",
    "        'metadata': {'category': 'technical', 'priority': 'medium'}\n",
    "    },\n",
    "    {\n",
    "        'id': 'art-008',\n",
    "        'text': \"\"\"Mobile App: Our mobile app is available on iOS (12.0+) and Android (8.0+). Download from the \n",
    "        App Store or Google Play. Mobile features include offline mode, push notifications, and biometric login. \n",
    "        Sync happens automatically when connected to wifi.\"\"\",\n",
    "        'metadata': {'category': 'mobile', 'priority': 'low'}\n",
    "    }\n",
    "]\n",
    "\n",
    "kb.add_documents(support_articles)\n",
    "print(\"✅ Knowledge base populated with support articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "demo_init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI provider initialized\n",
      "✅ Customer Support Assistant ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize AI providers\n",
    "# Note: Set OPENAI_API_KEY in your environment or .env file\n",
    "\n",
    "try:\n",
    "    primary_provider = OpenAIProvider()\n",
    "    print(\"✅ OpenAI provider initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  OpenAI initialization failed: {e}\")\n",
    "    print(\"   Using mock provider for demo\")\n",
    "    # You could implement a mock provider here for testing\n",
    "    primary_provider = None\n",
    "\n",
    "# Initialize resilient AI client\n",
    "if primary_provider:\n",
    "    ai_client = AIClient(primary_provider)\n",
    "    \n",
    "    # Initialize cost tracker\n",
    "    cost_tracker = CostTracker(daily_budget=5.0, monthly_budget=100.0)\n",
    "    \n",
    "    # Initialize secure assistant\n",
    "    assistant = SecureAIAssistant(ai_client, kb)\n",
    "    \n",
    "    print(\"✅ Customer Support Assistant ready!\")\n",
    "else:\n",
    "    print(\"⚠️  Cannot initialize assistant without API key\")\n",
    "    print(\"   Set OPENAI_API_KEY environment variable to use the demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "demo_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query handler ready\n"
     ]
    }
   ],
   "source": [
    "def handle_customer_query(question: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Handle customer support query with full AI pipeline\"\"\"\n",
    "    \n",
    "    if not primary_provider:\n",
    "        return {'error': 'AI provider not initialized'}\n",
    "    \n",
    "    # Check budget before processing\n",
    "    within_budget, budget_msg = cost_tracker.check_budget()\n",
    "    if not within_budget:\n",
    "        return {'error': budget_msg}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🤖 Processing Customer Query\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # System prompt for customer support\n",
    "    system_prompt = \"\"\"You are a helpful customer support assistant. Answer questions using the \n",
    "    provided knowledge base context. Be concise, friendly, and professional. If you cite information, \n",
    "    use [Source N] format. If you don't have enough information, say so and offer to escalate.\"\"\"\n",
    "    \n",
    "    # Get answer with RAG and security\n",
    "    start_time = time.time()\n",
    "    result = assistant.answer_question(question, system_prompt=system_prompt)\n",
    "    total_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    if 'error' in result:\n",
    "        if verbose:\n",
    "            print(f\"❌ Error: {result['error']}\")\n",
    "        return result\n",
    "    \n",
    "    # Track cost\n",
    "    # Note: We need to create a mock AIResponse for tracking\n",
    "    # In production, the assistant would return the full response object\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✅ Answer:\\n{result['answer']}\\n\")\n",
    "        print(f\"📊 Metrics:\")\n",
    "        print(f\"   Cost: ${result['cost']:.6f}\")\n",
    "        print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
    "        print(f\"   Tokens: {result['tokens']}\")\n",
    "        print(f\"   Total Time: {total_time:.0f}ms\")\n",
    "        \n",
    "        if result['security']['pii_redacted']:\n",
    "            print(f\"   🔒 PII Redacted: {result['security']['pii_redacted']}\")\n",
    "        \n",
    "        print(f\"\\n📚 Sources Retrieved: {len(result['sources'])}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"   {i}. {source['metadata'].get('category', 'N/A')} \"\n",
    "                  f\"(relevance: {1 - source['distance']:.2f})\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Query handler ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_test",
   "metadata": {},
   "source": [
    "### Test the Customer Support Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "test1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🤖 Processing Customer Query\n",
      "================================================================================\n",
      "Question: How do I reset my password?\n",
      "\n",
      "✅ Answer:\n",
      "To reset your password, click 'Forgot Password' on the login page. Enter your email address, and you'll receive a reset link within 5 minutes. Please note that the link expires after 24 hours. If you don't receive the email, check your spam folder or contact support for assistance [Source 1].\n",
      "\n",
      "📊 Metrics:\n",
      "   Cost: $0.000081\n",
      "   Latency: 3877ms\n",
      "   Tokens: 350\n",
      "   Total Time: 3912ms\n",
      "\n",
      "📚 Sources Retrieved: 3\n",
      "   1. authentication (relevance: 0.72)\n",
      "   2. onboarding (relevance: 0.23)\n",
      "   3. security (relevance: 0.21)\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic query\n",
    "if primary_provider:\n",
    "    result = handle_customer_query(\"How do I reset my password?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "test2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🤖 Processing Customer Query\n",
      "================================================================================\n",
      "Question: I can't log in with my email john.doe@example.com and phone 555-123-4567. Please help!\n",
      "\n",
      "✅ Answer:\n",
      "I’m sorry to hear that you're having trouble logging in. Here are a couple of steps you can try:\n",
      "\n",
      "1. **Password Reset**: If you can't remember your password, you can reset it by clicking 'Forgot Password' on the login page. Enter your email address, and you should receive a reset link within 5 minutes. Remember to check your spam folder if you don't see it [Source 1].\n",
      "\n",
      "2. **Two-Factor Authentication**: If you have Two-Factor Authentication (2FA) enabled, ensure you have access to your 2FA device for authentication. If you’ve lost access, please contact support with verification [Source 2].\n",
      "\n",
      "If you're still unable to log in after trying these steps, please let me know, and I can escalate this issue for further assistance.\n",
      "\n",
      "📊 Metrics:\n",
      "   Cost: $0.000143\n",
      "   Latency: 5882ms\n",
      "   Tokens: 469\n",
      "   Total Time: 5953ms\n",
      "   🔒 PII Redacted: ['email', 'phone']\n",
      "\n",
      "📚 Sources Retrieved: 3\n",
      "   1. authentication (relevance: 0.43)\n",
      "   2. security (relevance: 0.31)\n",
      "   3. technical (relevance: 0.21)\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Query with PII (will be redacted)\n",
    "if primary_provider:\n",
    "    result = handle_customer_query(\n",
    "        \"I can't log in with my email john.doe@example.com and phone 555-123-4567. Please help!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "test3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🤖 Processing Customer Query\n",
      "================================================================================\n",
      "Question: When will I be charged for my subscription?\n",
      "\n",
      "✅ Answer:\n",
      "You will be charged for your subscription on the 1st of each month for your subscription tier [Source 1].\n",
      "\n",
      "📊 Metrics:\n",
      "   Cost: $0.000058\n",
      "   Latency: 1100ms\n",
      "   Tokens: 315\n",
      "   Total Time: 1145ms\n",
      "\n",
      "📚 Sources Retrieved: 3\n",
      "   1. billing (relevance: 0.55)\n",
      "   2. onboarding (relevance: 0.37)\n",
      "   3. technical (relevance: 0.29)\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Billing question\n",
    "if primary_provider:\n",
    "    result = handle_customer_query(\"When will I be charged for my subscription?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "test4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🤖 Processing Customer Query\n",
      "================================================================================\n",
      "Question: What's the API rate limit for my tier?\n",
      "\n",
      "✅ Answer:\n",
      "The API rate limit for the standard tier is 1000 requests per hour, while for the enterprise tier, it is 10,000 requests per hour [Source 1].\n",
      "\n",
      "📊 Metrics:\n",
      "   Cost: $0.000065\n",
      "   Latency: 1791ms\n",
      "   Tokens: 328\n",
      "   Total Time: 1838ms\n",
      "\n",
      "📚 Sources Retrieved: 3\n",
      "   1. technical (relevance: 0.68)\n",
      "   2. billing (relevance: 0.22)\n",
      "   3. technical (relevance: 0.18)\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Technical integration question\n",
    "if primary_provider:\n",
    "    result = handle_customer_query(\"What's the API rate limit for my tier?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analytics",
   "metadata": {},
   "source": [
    "### View Analytics & Cost Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "view_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No usage data yet. Run some queries first!\n"
     ]
    }
   ],
   "source": [
    "# Display cost statistics\n",
    "if primary_provider and cost_tracker.calls:\n",
    "    stats = cost_tracker.get_stats()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 AI USAGE & COST ANALYTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal Calls: {stats['total_calls']}\")\n",
    "    print(f\"Total Cost: ${stats['total_cost']:.4f}\")\n",
    "    print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "    print(f\"Avg Latency: {stats['avg_latency_ms']:.0f}ms\")\n",
    "    print(f\"\\nToday's Cost: ${stats['today_cost']:.4f}\")\n",
    "    print(f\"Daily Budget Remaining: ${stats['today_budget_remaining']:.2f}\")\n",
    "    print(f\"\\nMonth Cost: ${stats['month_cost']:.4f}\")\n",
    "    print(f\"Monthly Budget Remaining: ${stats['month_budget_remaining']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nCost by Provider:\")\n",
    "    for provider, cost in stats['by_provider'].items():\n",
    "        print(f\"  {provider}: ${cost:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCost by Model:\")\n",
    "    for model, cost in stats['by_model'].items():\n",
    "        print(f\"  {model}: ${cost:.4f}\")\n",
    "    \n",
    "    # Show visualization\n",
    "    fig = cost_tracker.visualize_costs()\n",
    "    if fig:\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"No usage data yet. Run some queries first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt_engineering",
   "metadata": {},
   "source": [
    "## 6. Prompt Engineering Best Practices\n",
    "\n",
    "### Effective Prompt Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "prompt_examples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Structured Output Prompt\n",
      "================================================================================\n",
      "Task: Analyze the sentiment of: 'This product is amazing!'\n",
      "\n",
      "Output the result as JSON matching this schema:\n",
      "{\n",
      "  \"sentiment\": \"positive|negative|neutral\",\n",
      "  \"confidence\": 0.0-1.0,\n",
      "  \"key_phrases\": [\"string\"]\n",
      "}\n",
      "\n",
      "Important:\n",
      "- Output ONLY valid JSON, no explanation\n",
      "- All required fields must be present\n",
      "- Use null for missing optional fields\n",
      "\n",
      "================================================================================\n",
      "Example: Role-Based Prompt\n",
      "================================================================================\n",
      "You are a senior Python developer.\n",
      "\n",
      "Task: Review this code for security vulnerabilities\n",
      "\n",
      "Constraints:\n",
      "1. Focus on input validation\n",
      "2. Check for SQL injection risks\n",
      "3. Provide specific fix recommendations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PromptTemplates:\n",
    "    \"\"\"Reusable prompt templates for common tasks\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def structured_output(task: str, output_schema: str) -> str:\n",
    "        \"\"\"Prompt for structured JSON output\"\"\"\n",
    "        return f\"\"\"Task: {task}\n",
    "\n",
    "Output the result as JSON matching this schema:\n",
    "{output_schema}\n",
    "\n",
    "Important:\n",
    "- Output ONLY valid JSON, no explanation\n",
    "- All required fields must be present\n",
    "- Use null for missing optional fields\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def few_shot_classification(examples: List[Dict], text: str) -> str:\n",
    "        \"\"\"Few-shot learning for classification\"\"\"\n",
    "        examples_str = \"\\n\\n\".join([\n",
    "            f\"Input: {ex['input']}\\nOutput: {ex['output']}\"\n",
    "            for ex in examples\n",
    "        ])\n",
    "        \n",
    "        return f\"\"\"Classify the following text based on these examples:\n",
    "\n",
    "{examples_str}\n",
    "\n",
    "Now classify:\n",
    "Input: {text}\n",
    "Output:\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chain_of_thought(question: str) -> str:\n",
    "        \"\"\"Chain-of-thought reasoning\"\"\"\n",
    "        return f\"\"\"Question: {question}\n",
    "\n",
    "Let's solve this step by step:\n",
    "1. First, identify the key information\n",
    "2. Then, break down the problem\n",
    "3. Finally, provide the solution\n",
    "\n",
    "Show your reasoning for each step.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def role_based(role: str, task: str, constraints: List[str] = None) -> str:\n",
    "        \"\"\"Role-based prompting\"\"\"\n",
    "        prompt = f\"\"\"You are a {role}.\n",
    "\n",
    "Task: {task}\"\"\"\n",
    "        \n",
    "        if constraints:\n",
    "            prompt += \"\\n\\nConstraints:\\n\"\n",
    "            for i, constraint in enumerate(constraints, 1):\n",
    "                prompt += f\"{i}. {constraint}\\n\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "# Example usage\n",
    "print(\"Example: Structured Output Prompt\")\n",
    "print(\"=\"*80)\n",
    "schema = '''{\n",
    "  \"sentiment\": \"positive|negative|neutral\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"key_phrases\": [\"string\"]\n",
    "}'''\n",
    "print(PromptTemplates.structured_output(\n",
    "    \"Analyze the sentiment of: 'This product is amazing!'\",\n",
    "    schema\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example: Role-Based Prompt\")\n",
    "print(\"=\"*80)\n",
    "print(PromptTemplates.role_based(\n",
    "    role=\"senior Python developer\",\n",
    "    task=\"Review this code for security vulnerabilities\",\n",
    "    constraints=[\n",
    "        \"Focus on input validation\",\n",
    "        \"Check for SQL injection risks\",\n",
    "        \"Provide specific fix recommendations\"\n",
    "    ]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production",
   "metadata": {},
   "source": [
    "## 7. Production-Ready Patterns\n",
    "\n",
    "### Caching, Rate Limiting, Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "production_patterns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Production patterns implemented\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "from collections import deque\n",
    "from threading import Lock\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"LRU cache for AI responses\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.access_order = deque()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _make_key(self, messages: List[AIMessage], **kwargs) -> str:\n",
    "        \"\"\"Create cache key from messages and parameters\"\"\"\n",
    "        content = json.dumps([\n",
    "            {'role': m.role, 'content': m.content} for m in messages\n",
    "        ] + [kwargs])\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, messages: List[AIMessage], **kwargs) -> Optional[AIResponse]:\n",
    "        \"\"\"Get cached response\"\"\"\n",
    "        key = self._make_key(messages, **kwargs)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(key)\n",
    "            self.access_order.append(key)\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, messages: List[AIMessage], response: AIResponse, **kwargs):\n",
    "        \"\"\"Cache response\"\"\"\n",
    "        key = self._make_key(messages, **kwargs)\n",
    "        \n",
    "        # Evict oldest if at capacity\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            oldest = self.access_order.popleft()\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        self.cache[key] = response\n",
    "        self.access_order.append(key)\n",
    "    \n",
    "    def stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'max_size': self.max_size,\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'hit_rate': self.hits / total if total > 0 else 0\n",
    "        }\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute: int = 60):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.tokens = requests_per_minute\n",
    "        self.last_update = time.time()\n",
    "        self.lock = Lock()\n",
    "    \n",
    "    def acquire(self) -> bool:\n",
    "        \"\"\"Try to acquire a token\n",
    "        \n",
    "        Returns:\n",
    "            True if request allowed, False if rate limited\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            elapsed = now - self.last_update\n",
    "            \n",
    "            # Refill tokens based on time elapsed\n",
    "            self.tokens = min(\n",
    "                self.requests_per_minute,\n",
    "                self.tokens + elapsed * (self.requests_per_minute / 60)\n",
    "            )\n",
    "            self.last_update = now\n",
    "            \n",
    "            if self.tokens >= 1:\n",
    "                self.tokens -= 1\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "class ProductionAIClient(AIClient):\n",
    "    \"\"\"Production-ready AI client with caching and rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self, primary_provider: AIProviderInterface,\n",
    "                 fallback_provider: Optional[AIProviderInterface] = None,\n",
    "                 enable_cache: bool = True,\n",
    "                 rate_limit: int = 60):\n",
    "        super().__init__(primary_provider, fallback_provider)\n",
    "        self.cache = ResponseCache() if enable_cache else None\n",
    "        self.rate_limiter = RateLimiter(rate_limit)\n",
    "    \n",
    "    def complete(self, messages: List[AIMessage], **kwargs) -> AIResponse:\n",
    "        # Check rate limit\n",
    "        if not self.rate_limiter.acquire():\n",
    "            raise Exception(\"Rate limit exceeded\")\n",
    "        \n",
    "        # Check cache\n",
    "        if self.cache:\n",
    "            cached = self.cache.get(messages, **kwargs)\n",
    "            if cached:\n",
    "                print(\"✅ Cache hit!\")\n",
    "                return cached\n",
    "        \n",
    "        # Get response from provider\n",
    "        response = super().complete(messages, **kwargs)\n",
    "        \n",
    "        # Cache response\n",
    "        if self.cache:\n",
    "            self.cache.set(messages, response, **kwargs)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        return self.cache.stats() if self.cache else {}\n",
    "\n",
    "print(\"✅ Production patterns implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary & Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Architecture**\n",
    "   - Use abstraction layers for multi-provider support\n",
    "   - Implement failover and circuit breakers\n",
    "   - Design for observability from day one\n",
    "\n",
    "2. **Security**\n",
    "   - Always validate and sanitize inputs\n",
    "   - Redact PII before sending to APIs\n",
    "   - Implement content filtering\n",
    "   - Use rate limiting to prevent abuse\n",
    "\n",
    "3. **Cost Management**\n",
    "   - Track token usage and costs in real-time\n",
    "   - Set budgets and alerts\n",
    "   - Use caching to reduce API calls\n",
    "   - Choose appropriate models for each task\n",
    "\n",
    "4. **Performance**\n",
    "   - Implement response caching\n",
    "   - Use async/parallel processing where possible\n",
    "   - Monitor latency and set SLAs\n",
    "   - Consider edge caching for common queries\n",
    "\n",
    "5. **RAG Integration**\n",
    "   - Use vector databases for semantic search\n",
    "   - Chunk documents appropriately (300-500 tokens)\n",
    "   - Include metadata for better filtering\n",
    "   - Cite sources in responses\n",
    "\n",
    "6. **Prompt Engineering**\n",
    "   - Use system prompts to set context and constraints\n",
    "   - Provide examples for better accuracy (few-shot)\n",
    "   - Request structured output (JSON) when needed\n",
    "   - Iterate and test prompts with real data\n",
    "\n",
    "7. **Production Readiness**\n",
    "   - Implement comprehensive error handling\n",
    "   - Log all requests and responses\n",
    "   - Set up monitoring and alerting\n",
    "   - Have fallback strategies for failures\n",
    "   - Test at scale before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slides",
   "metadata": {},
   "source": [
    "## Interactive Presentation\n",
    "\n",
    "View the complete presentation with architecture diagrams and best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "open_slides",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Opening AI Integration Demo Presentation...\n",
      "   Path: /home/bbrelin/src/repos/salesforce/slides/ai_integration_demo.html\n",
      "✅ Presentation opened in browser\n",
      "   If browser didn't open, visit: file:///home/bbrelin/src/repos/salesforce/slides/ai_integration_demo.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 20:34:57.265: Not loading module \"atk-bridge\": The functionality is provided by GTK natively. Please try to not load it.\n"
     ]
    }
   ],
   "source": [
    "# Open the AI Integration Demo Presentation\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Path to the demo presentation\n",
    "demo_path = '/home/bbrelin/src/repos/salesforce/slides/ai_integration_demo.html'\n",
    "\n",
    "print(f\"✅ Opening AI Integration Demo Presentation...\")\n",
    "print(f\"   Path: {demo_path}\")\n",
    "\n",
    "# Launch in default browser using webbrowser module\n",
    "try:\n",
    "    # Use file:// URL for local files\n",
    "    file_url = f'file://{os.path.abspath(demo_path)}'\n",
    "    webbrowser.open(file_url, new=2)  # new=2 opens in a new tab if possible\n",
    "    print(f\"✅ Presentation opened in browser\")\n",
    "    print(f\"   If browser didn't open, visit: {file_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not open browser: {e}\")\n",
    "    print(f\"   Open manually: file://{os.path.abspath(demo_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv salesforce)",
   "language": "python",
   "name": "salesforce-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
